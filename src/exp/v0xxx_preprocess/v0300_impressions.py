# v0300_impressions.py (ƒê√£ s·ª≠a ƒë·ªïi)

import shutil
import sys
from pathlib import Path
from typing import Optional

import polars as pl
import typer
from loguru import logger
from typing_extensions import Annotated

from newsRecSys.utils._behaviors import create_binary_labels_column
from exputils.const import RAWDATA_DIRS, PREPROCESS_DIR
from exputils.utils import timer

# --- H·∫±ng s·ªë m·ªõi ---
NUM_CHUNKS = 10  # S·ªë l∆∞·ª£ng chunk √°p d·ª•ng cho t·∫•t c·∫£ c√°c splits
# --------------------

APP = typer.Typer(pretty_exceptions_enable=False)
FILE_NAME = Path(__file__).stem
OUTPUT_DIR = PREPROCESS_DIR / FILE_NAME
ARTICLES_DIR = PREPROCESS_DIR / "v0100_articles"
USERS_DIR = PREPROCESS_DIR / "v0200_users"


def prepare_output_dir(overwrite: bool | None):
    # (Gi·ªØ nguy√™n)
    if OUTPUT_DIR.exists():
        if overwrite or (overwrite is None and typer.confirm(f"Delete {OUTPUT_DIR}?")):
            logger.debug(f"Delete {OUTPUT_DIR}")
            shutil.rmtree(OUTPUT_DIR)
        else:
            logger.info(f"Skip to overwrite {OUTPUT_DIR}")
            sys.exit(0)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    return


def compute_impressions(
        lf_impressions: pl.LazyFrame,
        lf_history: pl.LazyFrame,
        lf_articles: pl.LazyFrame,
) -> pl.LazyFrame:
    """T√≠nh to√°n c√°c tr∆∞·ªùng c·∫ßn thi·∫øt cho impressions LazyFrame."""

    # üö® S·ª¨A ƒê·ªîI QUAN TR·ªåNG: ƒê√£ lo·∫°i b·ªè .with_row_index() v√¨ n√≥ ƒë∆∞·ª£c t·∫°o ·ªü main()
    lf_impressions = lf_impressions.with_columns(
        pl.col("user_id").cast(pl.Int32),
        # üö® ƒê·∫£m b·∫£o 'impression_index' ƒë√£ t·ªìn t·∫°i v√† ƒë√∫ng
        pl.col("impression_index").cast(pl.Int32),
    )
    # ... (C√°c logic ti·∫øp theo gi·ªØ nguy√™n) ...
    if "article_ids_clicked" not in lf_impressions.collect_schema().names():
        # ... (gi·ªØ nguy√™n)
        lf_impressions = lf_impressions.with_columns(
            pl.lit([]).cast(pl.List(pl.Int32)).alias("article_ids_clicked"),
            pl.lit(0).cast(pl.UInt16).alias("next_read_time"),
            pl.lit(0).cast(pl.UInt8).alias("next_scroll_percentage"),
        )

    # ... (C√°c logic Join, X·ª≠ l√Ω Inview, X·ª≠ l√Ω Clicked, K·∫øt h·ª£p gi·ªØ nguy√™n) ...
    lf_impressions = lf_impressions.join(
        lf_history.select("user_id", "user_index", "in_small"),
        on="user_id",
        validate="m:1",
    )
    lf_impressions_inview = (
        lf_impressions.select("impression_index", "article_ids_inview")
        .explode("article_ids_inview")
        .cast(pl.Int32)
        .join(
            lf_articles.select("article_id", "article_index"),
            left_on="article_ids_inview",
            right_on="article_id",
        )
        .group_by("impression_index", maintain_order=True)
        .agg(pl.col("article_index").alias("article_indices_inview"))
    )
    lf_impressions_click = (
        lf_impressions.select("impression_index", "article_ids_clicked")
        .explode("article_ids_clicked")
        .cast(pl.Int32)
        .join(
            lf_articles.select("article_id", "article_index"),
            left_on="article_ids_clicked",
            right_on="article_id",
            validate="m:1",
        )
        .group_by("impression_index", maintain_order=True)
        .agg(pl.col("article_index").alias("article_indices_clicked"))
    )
    lf_impressions = (
        lf_impressions.join(
            lf_impressions_inview,
            on="impression_index",
            how="left",
            validate="1:1",
        )
        .join(
            lf_impressions_click,
            on="impression_index",
            how="left",
            validate="1:1",
        )
        .pipe(
            create_binary_labels_column,  # type: ignore
            clicked_col="article_indices_clicked",
            inview_col="article_indices_inview",
            shuffle=False,
            seed=123,
        )
        .with_columns(
            (pl.col("impression_time").dt.timestamp() // 10 ** 6)
            .cast(pl.Int32)
            .alias("impression_ts"),
        )
        # B·ªè sort("impression_index") ƒë·ªÉ gi·ªØ t√≠nh lazy, ch·ªâ sort khi c·∫ßn thi·∫øt
        .select(
            pl.col("impression_index"),
            pl.col("impression_id"),
            pl.col("impression_ts"),
            pl.col("impression_time"),
            pl.col("user_index"),
            pl.col("session_id"),
            pl.col("read_time").fill_null(0).cast(pl.UInt16),
            pl.col("scroll_percentage").fill_null(0).cast(pl.UInt8),
            pl.col("device_type").cast(pl.Int8),
            pl.col("is_sso_user").cast(bool),
            pl.col("gender").fill_null(-1).cast(pl.Int8),
            pl.col("postcode").fill_null(-1).cast(pl.Int8),
            pl.col("age").fill_null(-1).cast(pl.Int8),
            pl.col("is_subscriber").cast(bool),
            pl.col("next_read_time").fill_null(0).cast(pl.UInt16),
            pl.col("next_scroll_percentage").fill_null(0).cast(pl.UInt8),
            pl.col("article_indices_inview").fill_null(pl.lit([])),
            pl.col("article_indices_clicked").fill_null(pl.lit([])),
            pl.col("in_small").cast(bool),
            pl.col("labels"),
        )
    )
    return lf_impressions


@APP.command()
def main(
        overwrite: Annotated[Optional[bool], typer.Option("--overwrite/--skip")] = None,
):
    prepare_output_dir(overwrite=overwrite)

    lf_articles = pl.scan_parquet(ARTICLES_DIR / "dataset.parquet")

    for split in ["train", "validation", "test"]:

        impressions_file = RAWDATA_DIRS[split] / "behaviors.parquet"
        lf_history = pl.scan_parquet(USERS_DIR / split / "dataset.parquet")
        output_path_final = OUTPUT_DIR / split / "dataset.parquet"
        output_path_final.parent.mkdir(parents=True, exist_ok=True)

        logger.info(f"*** B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {split} v·ªõi {NUM_CHUNKS} chunks ***")

        # 1. Qu√©t LazyFrame G·ªêC v√† T·∫†O CH·ªà M·ª§C TO√ÄN C·ª§C M·ªòT L·∫¶N
        lf_full = pl.scan_parquet(impressions_file).with_row_index(
            name="impression_index"
        )

        with timer(f"Get total rows for chunking ({split})"):
            n_rows_input = lf_full.select(pl.len()).collect().item()

        chunk_size = (n_rows_input + NUM_CHUNKS - 1) // NUM_CHUNKS

        logger.info(f"T·ªïng s·ªë h√†ng: {n_rows_input}. K√≠ch th∆∞·ªõc m·ªói chunk: {chunk_size}")

        processed_chunks = []

        for i in range(NUM_CHUNKS):
            offset = i * chunk_size
            limit = chunk_size

            if offset >= n_rows_input:
                break

            logger.info(f"Processing chunk {i + 1}/{NUM_CHUNKS} (offset={offset}, limit={limit})...")

            # 2. Slicing LazyFrame (ƒë√£ c√≥ ch·ªâ m·ª•c)
            lf_chunk = lf_full.slice(offset, limit)

            # 3. Th·ª±c hi·ªán t√≠nh to√°n
            lf_output_chunk = compute_impressions(
                lf_impressions=lf_chunk,  # Lf_chunk ƒë√£ c√≥ 'impression_index' ƒë√∫ng
                lf_history=lf_history,
                lf_articles=lf_articles,
            )

            with timer(f"Collect chunk {i + 1}"):
                df_output_chunk = lf_output_chunk.collect(engine="streaming")

            processed_chunks.append(df_output_chunk)
            logger.info(f"Chunk {i + 1} shape: {df_output_chunk.shape}")

        # 4. Gh√©p n·ªëi v√† Ghi k·∫øt qu·∫£ cu·ªëi c√πng
        if not processed_chunks:
            logger.warning(f"No data processed for split {split}. Output file will not be created.")
            continue

        with timer(f"Concatenate and write final output ({split})"):
            # Gh√©p n·ªëi c√°c DataFrames ƒë√£ x·ª≠ l√Ω
            df_output_final = pl.concat(processed_chunks).sort("impression_index")  # Sort l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±

            df_output_final.write_parquet(output_path_final, compression="zstd", use_pyarrow=True)

            logger.info(f"Finished. Final output shape: {df_output_final.shape}. Saved to {output_path_final}")
            logger.info(df_output_final.head(5))

        # --- Ki·ªÉm tra t√≠nh nh·∫•t qu√°n (Test consistency) ---
        with timer(f"Test consistency ({split})"):
            if not output_path_final.exists():
                logger.warning(f"Skipping consistency check for {split}: Output file not found.")
                continue

            lf_output_check = pl.scan_parquet(output_path_final)
            n_rows_output_check = lf_output_check.select(pl.len()).collect().item()

            assert n_rows_input == n_rows_output_check, (
                f"D·ªØ li·ªáu {split} kh√¥ng kh·ªõp s·ªë d√≤ng: Input={n_rows_input}, Output={n_rows_output_check}"
            )
            logger.info(f"Consistency check passed for {split}: Input and output row counts match.")


if __name__ == "__main__":
    APP()